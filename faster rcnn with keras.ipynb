{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, TimeDistributed, Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Print the process or not\n",
    "        self.verbose = True\n",
    "        \n",
    "        # Name of base network\n",
    "        self.network = 'vgg'\n",
    "        \n",
    "        # Setting for data augmentation\n",
    "        self.use_horizontal_flips = False\n",
    "        self.use_vertical_flips = False\n",
    "        self.rot_90 = False\n",
    "        \n",
    "        # Anchor box scales\n",
    "    # Note that if im_size is smaller, anchor_box_sclase should be scaled\n",
    "    # Original anchor_box_scales in the paper is [128, 256, 512]\n",
    "        self.anchor_box_scales = [64, 128, 256]\n",
    "        \n",
    "        # Anchor box ratios\n",
    "        self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]] \n",
    "        \n",
    "        # Size to resize the smallest side of the image\n",
    "        # Original setting in paper is 600. Set to 300 in here to save training time\n",
    "        self.im_size = 300\n",
    "        \n",
    "        # image channel-wise mean to subtract\n",
    "        self.img_channel_mean = [103.939, 116.779, 123.68]\n",
    "        self.img_scaling_factor = 1.0\n",
    "        \n",
    "        # number of ROIs at once\n",
    "        self.num_rois = 4\n",
    "        \n",
    "        # stride at the RPN (this depends on the network configuration)\n",
    "        self.rpn_stride = 16\n",
    "        \n",
    "        self.balanced_classes = False \n",
    "        \n",
    "        # scaling the stdev\n",
    "        self.std_scaling = 4.0\n",
    "        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "        \n",
    "        # overlaps for RPN\n",
    "        self.rpn_min_overlap = 0.3\n",
    "        self.rpn_max_overlap = 0.7\n",
    "        \n",
    "        # overlaps for classifier ROIs\n",
    "        self.classifier_min_overlap = 0.1\n",
    "        self.classifier_max_overlap = 0.5\n",
    "        \n",
    "        # placeholder for the class mapping, automatically generated by the parser\n",
    "        self.class_mapping = None\n",
    "        \n",
    "        self.model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser the data from annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(input_path):\n",
    "    \"\"\"Parse the data from annotation file\n",
    "    \n",
    "    Args:\n",
    "        input_path: annotation file path\n",
    "        \n",
    "    Returns:\n",
    "        all_data: list(filepath, width, height, list(bboxes))\n",
    "        classes_count: dict{key:class_name, value:count_num}\n",
    "            e.g. {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745}\n",
    "        class_mapping: dict{ket:class_name, value: idx}\n",
    "            e.g. {'Car', 0, 'Mobile phone': 1, 'Person': 2}\n",
    "    \"\"\"\n",
    "    found_bg = False\n",
    "    all_imgs = {}\n",
    "    \n",
    "    classes_count = {}\n",
    "    \n",
    "    class_mapping = {}\n",
    "    \n",
    "    visualise = True\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    with open(input_path, 'r') as f:\n",
    "        \n",
    "        print('Parsing annotation files')\n",
    "        \n",
    "        for line in f:\n",
    "            \n",
    "            # Print process\n",
    "            sys.stdout.write('\\r'+'idx=' + str(i))\n",
    "            i += 1\n",
    "            \n",
    "            line_split = line.strip().split(',')\n",
    "            \n",
    "            # Make sure the info saved in annotation file matching the format (path_filename, x1, y1, x2, y2, class_name)\n",
    "            # Note:\n",
    "            #   One path_filename might has several classes (class_name)\n",
    "            #   x1, y1, x2, y2 are the pixel value of the original image, not the ratio value\n",
    "            #   (x1, y1) top left coordinates; (x2, y2) bottom right coordinates\n",
    "            \n",
    "            (filename,x1,y1,x2,y2,class_name) = line_split\n",
    "            \n",
    "            if class_name not in classes_count:\n",
    "                classes_count[class_name] = 1\n",
    "            else:\n",
    "                classes_count[class_name] += 1\n",
    "            \n",
    "            if class_name not in class_mapping:\n",
    "                if class_name == 'bg' and found_bg == False:\n",
    "                    print('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n",
    "                    found_bg = True\n",
    "                class_mapping[class_name] = len(class_mapping)\n",
    "                \n",
    "            if filename not in all_imgs:\n",
    "                all_imgs[filename] = []\n",
    "                \n",
    "                img = cv2.imread(filename)\n",
    "                (rows,cols) = img.shape[:2]\n",
    "                all_imgs[filename]['filepath'] = filename\n",
    "                all_imgs[filename]['width'] = cols\n",
    "                all_imgs[filename]['height'] = rows\n",
    "                all_imgs[filename]['bboxes'] = []\n",
    "                # if np.random.randint(0,6) > 0:\n",
    "                #     all_imgs[filename]['imageset'] = 'trainval'\n",
    "                # else:\n",
    "                #     all_imgs[filename]['imageset'] = 'test'\n",
    "                \n",
    "            all_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'y1': int(y1), 'x2': int(x2), 'y2': int(y2)})\n",
    "            \n",
    "        all_data = []\n",
    "        for key in all_imgs:\n",
    "            all_data.append(all_imgs[key]) # dict 만든 다음에 list로 바꾸는게 더 메모리 효율적인가?\n",
    "            \n",
    "        # make sure the bg class is last in the list\n",
    "        if found_bg:\n",
    "            if class_mapping['bg'] != len(class_mapping) - 1:\n",
    "                key_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n",
    "                val_to_switch = class_mapping['bg']\n",
    "                class_mapping['bg'] = len(class_mapping) - 1\n",
    "                class_mapping[key_to_switch] = val_to_switch\n",
    "                \n",
    "        return all_data, classes_count, class_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ROI Pooling Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> permute_dimensions 할때 (0,4,1,2,3)이어야할 것 같다. 그래야 doc_string에서 설명하는 output과 모양이 같아진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoiPoolingConv(Layer):\n",
    "    '''ROI pooling layer for 2D inputs.\n",
    "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
    "    K. He, X. Zhang, S. Ren, J. Sun\n",
    "    # Arguments\n",
    "        pool_size: int\n",
    "            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n",
    "        num_rois: number of regions of interest to be used\n",
    "    # Input shape\n",
    "        list of two 4D tensors [X_img,X_roi] with shape:\n",
    "        X_img:\n",
    "        `(1, rows, cols, channels)`\n",
    "        X_roi:\n",
    "        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
    "    # Output shape\n",
    "        3D tensor with shape:\n",
    "        `(1, num_rois, channels, pool_size, pool_size)`\n",
    "    '''\n",
    "    def __init__(self, pool_size, num_rois, **kwargs):\n",
    "        \n",
    "        self.dim_ordering = K.image_dim_ordering()\n",
    "        self.pool_size = pool_size\n",
    "        slef.num_rois = num_rois\n",
    "        \n",
    "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        assert(isinstance(input_shape, list)) # 내가 추가함\n",
    "        self.nb_channels = input_shape[0][3]\n",
    "        \n",
    "        # super(RoiPoolingConv, self).build(input_shape) # 이건 왜 안하지? 공식문서에서는 하라는데?\n",
    "        \n",
    "    def comput_output_shape(self, input_shape):\n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "        \n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        assert(len(x) == 2)\n",
    "        \n",
    "        # X[0] is image with shape (batch_size, rows, cols, channels)\n",
    "        img = x[0]\n",
    "        \n",
    "        # x[1] is roi with shape (batch_size, num_rois, 4) with ordering (x,y,w,h)\n",
    "        rois = x[1]d\n",
    "        \n",
    "        input_shape = K.shape(img)\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        for roi_idx in range(self.num_rois):\n",
    "            \n",
    "            x = rois[0, roi_idx, 0]\n",
    "            y = rois[0, roi_idx, 1]\n",
    "            w = rois[0, roi_idx, 2]\n",
    "            h = rois[0, roi_idx, 3]\n",
    "            \n",
    "            x = K.cast(x, 'int32')\n",
    "            y = K.cast(y, 'int32')\n",
    "            w = K.cast(w, 'int32')\n",
    "            h = K.cast(h, 'int32')\n",
    "            \n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            rs = tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size)) # 왜 그냥 resize를 쓰지??\n",
    "            outputs.append(rs)\n",
    "            \n",
    "        final_output = K.concatenate(outputs, axis=0)\n",
    "        \n",
    "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "        # Might be (1, 4, 7, 7, 3)\n",
    "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
    "        \n",
    "        # permute_dimensions is similar to transpose\n",
    "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4)) # 이건 왜 하는걸까?\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'pool_size': self.pool_size,\n",
    "                  'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items() + list(config.items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vgg-16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_output_length(width, height):\n",
    "    def get_output_length(input_length):\n",
    "        return input_length//16\n",
    "    \n",
    "    return get_output_length(width), get_output_length(height)\n",
    "\n",
    "def nn_base(input_tensor=None, trainable=False):\n",
    "    \n",
    "    input_shape = (None, None, 3)\n",
    "    \n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = input_tensor\n",
    "            \n",
    "    bn_axis = 3\n",
    "    \n",
    "    # Block 1\n",
    "    x = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv1\")(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", name=\"block1_conv2\")(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block1_pool\")(x)\n",
    "    \n",
    "    # Block 2 \n",
    "    x = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv1\")(x)\n",
    "    x = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\", name=\"block2_conv2\")(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block2_pool\")(x)\n",
    "    \n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv1\")(x)\n",
    "    x = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv2\")(x)\n",
    "    x = Conv2D(256, (3, 3), activation=\"relu\", padding=\"same\", name=\"block3_conv3\")(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block3_pool\")(x)\n",
    "    \n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv1\")(x)\n",
    "    x = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv2\")(x)\n",
    "    x = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block4_conv3\")(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name=\"block4_pool\")\n",
    "    \n",
    "    # Block5\n",
    "    x = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv1\")(x)\n",
    "    x = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv2\")(x)\n",
    "    x = Conv2D(512, (3, 3), activation=\"relu\", padding=\"same\", name=\"block5_conv3\")(x)\n",
    "#     x = Maxpooling2D((2, 2), strides=(2, 2), name=\"block5_pool\")(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpn_layer(base_layers, num_anchors):\n",
    "    \"\"\"Create a rpn layer\n",
    "        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer\n",
    "                Keep the padding 'same' to preserve the feature map's size\n",
    "        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer\n",
    "                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output\n",
    "                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation\n",
    "        Args:\n",
    "            base_layer: vgg in here\n",
    "            num_anchors: 9 in here\n",
    "            \n",
    "        Returns:\n",
    "            [x_class, x_regr, base_layer]\n",
    "            x_class: classification for whether it's an object\n",
    "            x_regr: bboxes regresssion\n",
    "            base_layers: vgg in here\n",
    "        \"\"\"\n",
    "    x = Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\", kernel_initializer=\"normal\", name=\"rpn_conv1\")(base_layers)\n",
    "    \n",
    "    x_class = Conv2D(num_anchors, (1, 1), activation=\"sigmoid\", kernel_initializer=\"uniform\", name=\"rpn_out_class\")(x)\n",
    "    x_regr = Conv2D(num_anchors * 4, (1, 1), activation=\"linear\", kernel_initializer=\"zero\", name=\"rpn_out_regress\")(x)\n",
    "    \n",
    "    return [x_class, x_regr, base_layers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifer layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 이 classifer는 object를 분류하는 역할이다.  \n",
    "rcnn에서는 물체의 여부를 가르는 binary classifier와 분류하는 mulit classifier로 2개가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_layer(base_layers, input_rois, num_rois, nb_classes=4):\n",
    "    \"\"\"Create a classifier layer\n",
    "    \n",
    "    Args:\n",
    "        base_layers: vgg\n",
    "        input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
    "        num_rois: number of rois to be processed in one time (4 in here)\n",
    "        \n",
    "    Returns:\n",
    "        list(out_class, out_regr)\n",
    "        out_class: classifier layer output\n",
    "        out_regr:regression layer output\n",
    "    \"\"\"\n",
    "    \n",
    "    input_shape = (num_rois,7,7,512)\n",
    "    \n",
    "    pooling_regions = 7\n",
    "    \n",
    "    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n",
    "    # num_rois (4) 7x7 roi pooling\n",
    "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layer, input_rois])\n",
    "    \n",
    "    # Flatten the convolutional layer and connected to 2 FC and 2 dropout\n",
    "    out = TimeDistributed(Flatten(name=\"flatten\"))(out_roi_pool)\n",
    "    out = TimeDistributed(Dense(4096, activation=\"relu\", name=\"fc1\"))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "    out = TimeDistributed(Dense(4096, activation=\"relu\", name=\"fc2\"))(out)\n",
    "    out = TimeDistributed(Dropout(0.5))(out)\n",
    "    \n",
    "    # There are two output layer\n",
    "    # out_class: softmax activation function for classify the class name of the object\n",
    "    # out_regr: linear activation function for bboxes coordinates regression\n",
    "    out_class = TimeDistributed(Dense(nb_classes, activation=\"softmax\", kernel_initializer=\"zero\"), name=\"dense_class_{}\".format(nb_classes))(out)\n",
    "    # note: no regression target for bg class\n",
    "    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation=\"linear\", kernel_initializer=\"zero\"), name=\"dense_regress_{}\".format(nb_classes))(out)\n",
    "    \n",
    "    return [out_class, out_regr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate IoU (Intersection of Union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 여기서는 w, h가 width, height의 길이가 아니라 사각형의 오른쪽 아래 모서리 좌표인 것 같다. 그래야 수식이 말이 된다. 그렇게한 이유는 아직 잘 모르겠다. 하지만 오해하기 쉬운것같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union(au, bu, area_intersection):\n",
    "    area_a = (au[2] - au[0]) * (au[3] - au[1])\n",
    "    area_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n",
    "    aread_union = area_a + area_b - area_intersection\n",
    "    return area_union\n",
    "\n",
    "\n",
    "def intersection(ai, bi):\n",
    "    x = max(ai[0], bi[0])\n",
    "    y = max(ai[1], bi[1])\n",
    "    w = min(ai[2], bi[2]) - x\n",
    "    h = min(ai[3], bi[3]) - y\n",
    "    if w < 0 or h < 0:\n",
    "        return 0\n",
    "    return w*h\n",
    "\n",
    "\n",
    "def iou(a, b):\n",
    "    # a and b shoud be (x1,y1,x2,y2)\n",
    "    \n",
    "    if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n",
    "        return 0.0\n",
    "    \n",
    "    area_i = intersection(a, b)\n",
    "    area_u = union(a, b, area_i)\n",
    "    \n",
    "    return float(area_i) / float(area_u + 1e-6) # 분모가 0이 될 수 있으니 스무딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the rpn for all anchors of all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n",
    "    \"\"\"(Important part!) Calculate the rpn for all anchors\n",
    "        If feature map has shape 38x50=1900, there are 1900x9=17100 potential anchors\n",
    "        \n",
    "    Args:\n",
    "        C: config\n",
    "        img_data: augmented image data\n",
    "        width: original image width (e.g. 600)\n",
    "        height: original image height (e.g. 800)\n",
    "        resized_width: resized image width according to C.im_size (e.g.300)\n",
    "        resized_height: resized image height accordign to c.im_size (e.g.400)\n",
    "        img_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size\n",
    "        \n",
    "    Returns:\n",
    "        y_rpn_cls: list(num_bboxes, y_is_box_valid + y_rpn_overlap)\n",
    "            y_is_box_valid: 0 or 1 (0 means the box is invalid, 1 means the box is valid)\n",
    "            y_rpn_overlap: 0 or 1 (0 means the box is not an object, 1 means the box is an object)\n",
    "        y_rpn_regr: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)\n",
    "            y_rpn_regr: x1,y1,x2,y2 bounding boxes coordinates\n",
    "    \"\"\"\n",
    "    downscale = float(C.rpn_stride)\n",
    "    anchor_sizes = C.anchor_box_scales # 128, 256, 512\n",
    "    anchor_ratios = C.anchor_box_ratios # 1:1, 1:2*sqrt(2), 2*sqrt(2):1\n",
    "    num_anchors = len(anchor_sizes) * len(anchor_ratios) # 3x3=9\n",
    "    \n",
    "    # calculate the output map size based on the network architecture\n",
    "    (output_width, output_height) = img_length_calc_function(resized_width, resiezed_height)\n",
    "    \n",
    "    n_anchratios = len(anchor_ratios) # 3\n",
    "    \n",
    "    # initialise empty output objectives\n",
    "    y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n",
    "    y_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n",
    "    y_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n",
    "    \n",
    "    num_bboxes = len(img_data['bboxes'])\n",
    "    \n",
    "    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n",
    "    best_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n",
    "    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n",
    "    best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n",
    "    best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n",
    "     \n",
    "    # get the GT box coordinates, and resize to account for image resizing\n",
    "    gta = np.zeros((num_bboxes, 4))\n",
    "    for bbox_num, bbox in enumerate(img_data[\"bboxes\"]):\n",
    "        #get the Ground Truth box coordinates, and resize to account for image resizing\n",
    "        gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n",
    "        gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n",
    "        gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(heigt))\n",
    "        gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n",
    "        \n",
    "    # rpn ground truth\n",
    "    \n",
    "    for anchor_size_idx in range(len(anchor_sizes)):\n",
    "        for anchor_ratio_idx in range(n_anchratios):\n",
    "            anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n",
    "            anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\n",
    "            \n",
    "            for ix in range(output_width):\n",
    "                # x-coordinates of the current anchor box\n",
    "                x1_anc = downscale * (ix + 0.5) - anchor_x / 2\n",
    "                x2_anc = downscale * (ix + 0.5) + anchor_x / 2\n",
    "                \n",
    "                # ignore boxes that go across image boundaries\n",
    "                if x1_anc < 0 or x2_anc > resized_width:\n",
    "                    continue\n",
    "                \n",
    "                for jy in range(output_height):\n",
    "                    \n",
    "                    # y_coordinates of the current anchor box\n",
    "                    y1_anc = downscale * (jy + 0.5) - anchor_y / 2\n",
    "                    y2_anc = downscale * (jy + 0.5) + anchor_y / 2\n",
    "                    \n",
    "                    # ignore boxes that go across image boundaries\n",
    "                    if y1_anc < 0 or y2_anc > resized_height:\n",
    "                        continue\n",
    "                        \n",
    "                    # bbox_type indicates wheter an anchor should be a target\n",
    "                    # Initialize with 'negative'\n",
    "                    bbox_type = \"neg\"\n",
    "                    \n",
    "                    # this is the best IOU for the (x,y) coord and the current anchor\n",
    "                    # note that this is differnet from the best IOU for a GT bbox\n",
    "                    best_iou_for_loc = 0.0\n",
    "                    \n",
    "                    for bbox_num in range(num_bboxes):\n",
    "                        \n",
    "                        # get IOU of the current GT box and the current anchor box\n",
    "                        curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n",
    "                        # calcualte the regression targets if they will be needed\n",
    "                        if curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n",
    "                            cx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n",
    "                            cy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n",
    "                            cxa = (x1_anc + x2_anc) / 2.0\n",
    "                            cya = (y1_anc + y2_anc) / 2.0\n",
    "                            \n",
    "                            # x, y are the center point of ground-truth box\n",
    "                            # xa,ya are the center point of anchor bbox (xa=downscale * ( ix + 0.5); ya=downscale * (iy+0.5))\n",
    "                            # w,h are the width and height and height fo ground-truth bbox\n",
    "                            # wa,ha are the width and height of anchor bbox\n",
    "                            # tx = (x - xa) / wa\n",
    "                            # ty = (y - ya) / ha\n",
    "                            # tw = log(w / wa)\n",
    "                            # th = log(h / ha)\n",
    "                            tx = (cx - cxa) / (x2_anc - x1_anc)\n",
    "                            ty = (cy - cya) / (y2_anc - y1_anc)\n",
    "                            tw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n",
    "                            th = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n",
    "                            \n",
    "                        if img_data[\"bboxes\"][bbox_num]['class'] ! = \"bg\":\n",
    "                            \n",
    "                            # all GT boxes should be mapped to an anchor box, so we keep track of which anchor was best\n",
    "                            if curr_iou > best_iou_for_bbox[bbox_num]:\n",
    "                                best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n",
    "                                best_iou_for_bbox[bbox_num] = curr_iou\n",
    "                                best_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]\n",
    "                                best_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]\n",
    "                                   \n",
    "                            # we set the anchor to positive if the IOU is > 0.7 (it does not matter if there was another better box, it just idicates overlap)\n",
    "                            if curr_iou > C.rpn_max_overlap:\n",
    "                                bbox_type = \"pos\"\n",
    "                                num_anchors_for_bbox[bbox_num] += 1  \n",
    "                                # we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n",
    "                                if curr_iou > best_iou_for_loc:\n",
    "                                    best_iou_for_loc = curr_iou\n",
    "                                    best_regr = (tx, ty, tw, th)\n",
    "                                    \n",
    "                            # if the IOU is > 0.3 and < 0.7, it is ambiguous and no included in the objective\n",
    "                            if C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n",
    "                                # gray zone between neg and pos\n",
    "                                if bbox_type != \"pos\":\n",
    "                                    bbox_type = 'neutral'\n",
    "                                    \n",
    "                    # turn on or off ouputs depending on IOUs\n",
    "                    if bbox_type == \"neg\":\n",
    "                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "                    elif bbox_type == \"neutral\":\n",
    "                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "                    elif bbox_type == \"pos\":\n",
    "                        y_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "                        y_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "                        start = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n",
    "                        y_rpn_regr[jy, ix, start:start+4] = best_regr\n",
    "                        \n",
    "    # we ensure that every bbox has at least one positive RPN region\n",
    "    \n",
    "    for idx in range(num_anchors_for_bbox.shape[0]):\n",
    "        if num_anchors_for_bbox[idx] == 0:\n",
    "            # no box with an IOU greater than zero ...\n",
    "            if best_anchor_for_bbox[idx, 0] == -1:\n",
    "                continue\n",
    "            y_is_box_valid[\n",
    "                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchorratios *\n",
    "                best_anchor_for_bbox[idx,3]] = 1\n",
    "            y_rpn_overlap[\n",
    "                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchorratios *\n",
    "                best_anchor_for_bbox[idx,3]] = 1\n",
    "            start = 4 * (best_anchor_for_bbox[idx,2] + n_anchroatios * best_anchor_for_bbox[idx,3])\n",
    "            y_rpn_regr[\n",
    "                best_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n",
    "            \n",
    "    y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n",
    "    y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n",
    "    \n",
    "    y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n",
    "    y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n",
    "    \n",
    "    y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n",
    "    y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n",
    "    \n",
    "    pos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n",
    "    neg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n",
    "    \n",
    "    num_pos = len(pos_locs[0])\n",
    "    \n",
    "    # one issue is that the RPN has many more negative than positive regions, os we turn off some of the negative\n",
    "    # regions, We also limit it to 256 regions.\n",
    "    num_regions = 256\n",
    "    \n",
    "    if len(pos_locs[0]) > num_regions/2:\n",
    "        val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n",
    "        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
    "        num_pos = num_regions/2\n",
    "        \n",
    "    if len(neg_locs[0]) + num_pos > num_regions:\n",
    "        val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n",
    "        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
    "        \n",
    "    y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n",
    "    y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n",
    "    \n",
    "    return np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get new image size and augment the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> augment한 후에 augment된 이미지를 저장하고 filepath도 img_data_aug에 업데이트해야 하지 않을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_img_size(width, height, img_min_side=300):\n",
    "    if wdith <= height:\n",
    "        f = float(img_min_side) / width\n",
    "        resized_height = int(f * height)\n",
    "        resized_width = img_min_side\n",
    "    else:\n",
    "        f = float(img_min_side) / height\n",
    "        resized_width = int(f * width)\n",
    "        resized_height = img_min_side\n",
    "        \n",
    "    return resized_width, resized_hegiht\n",
    "\n",
    "def augment(img_data, config, augment=True):\n",
    "    assert 'filepath' in img_data\n",
    "    assert 'bboxes' in img_data\n",
    "    assert 'width' in img_data\n",
    "    assert 'height' in img_data\n",
    "    \n",
    "    img_data_aug = copy.deepcopy(img_data)\n",
    "    \n",
    "    img = cv2.imread(img_data_aug['filepath'])\n",
    "    \n",
    "    if augment:\n",
    "        rows, cols = img.shape[:2]\n",
    "        \n",
    "        if config.use_horizontal_flips and np.random.randint(0, 2) == 2:\n",
    "            img = cv2.flip(img, 1)\n",
    "            for bbox in img_data_aug['bboxes']:\n",
    "                x1 = bbox['x1']\n",
    "                x2 = bbox['x2']\n",
    "                bbox['x2'] = cols - x1\n",
    "                bbox['x1'] = cols - x2\n",
    "                \n",
    "        if config.use_vertical_flips and np.random.randint(0, 2) == 0:\n",
    "            img = cv2.flip(img, 0)\n",
    "            for bbox in img_data_aug['bboxes']:\n",
    "                y1 = bbox['y1']\n",
    "                y2 = bbox['y2']\n",
    "                bbox['y2'] = rows - y1\n",
    "                bbox['y1'] = rows - y2\n",
    "                \n",
    "        if config.rot_90:\n",
    "            angle = np.random.choice([0,90,180,270], 1)[0]\n",
    "            if angle == 270:\n",
    "                img = np.transpose(img, (1,0,2))\n",
    "                img = cv2.flip(img, 0)\n",
    "            elif angle == 180:\n",
    "                img = cv2.flip(img, -1)\n",
    "            elif angle == 90:\n",
    "                img = np.transpose(img, (1,0,2))\n",
    "                img = cv2.flip(img, 1)\n",
    "            elif angle == 0:\n",
    "                pass\n",
    "            \n",
    "            for bbox in img_data_aug['bboxes']:\n",
    "                x1 = bbox['x1']\n",
    "                x2 = bbox['x2']\n",
    "                y1 = bbox['y1']\n",
    "                y2 = bbox['y2']\n",
    "                if angle == 270:\n",
    "                    bbox['x1'] = y1\n",
    "                    bbox['x2'] = y2\n",
    "                    bbox['y1'] = cols - x2\n",
    "                    bbox['y2'] = cols - x1\n",
    "                elif angle == 180:\n",
    "                    bbox['x1'] = cols - x2\n",
    "                    bbox['x2'] = cols - x1\n",
    "                    bbox['y1'] = rows - y2\n",
    "                    bbox['y2'] = rows - y1\n",
    "                elif angle == 90:\n",
    "                    bbox['x1'] = rows - y2\n",
    "                    bbox['x2'] = rows - y1\n",
    "                    bbox['y1'] = x1\n",
    "                    bbox['y2'] = x2\n",
    "                elif angle == 0:\n",
    "                    pass\n",
    "                \n",
    "    img_data_aug['width'] = img.shape[1]\n",
    "    img_data_aug['height'] = img.shape[0]\n",
    "    return img_data_aug, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
